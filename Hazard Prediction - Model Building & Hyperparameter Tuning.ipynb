{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Hazard Ratings for a Maintenance Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building and Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As we know the dataset contains the count variables and the response variables is poisson distributed, we will try the poisson regression models such as lasso, Hist Gradient Booster, Poisson Regressor, XGB Regressor with count as poisson, Tweedie Regressor. Also, we will try with DecisionTree and RandomForest regressor as well.\n",
    "We will evaluate the model on MAE (mean absolute error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.linear_model import PoissonRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import TweedieRegressor\n",
    "import xgboost as xg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> import the pickle file created in feature transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'transformed_all_data_hazard'\n",
    "infile = open(filename,'rb')\n",
    "all_data = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50999, 23)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Original Train- Test data seperation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40799, 22), (10200, 21))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=all_data.drop('data',1)[all_data['data']=='train']\n",
    "test=all_data.drop(['data','Hazard'],1)[all_data['data']=='test']\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "> Separating training and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27335, 21), (13464, 21), (27335,), (13464,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(train.drop('Hazard', axis=1), train['Hazard'], test_size=0.33, random_state=2)\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As we checked in EDA, the DV is skewed and we will apply squareroot transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train**(1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We will create a estimator function to check the performance with different models, and will select one with better score and then we can do hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimator(x,y,z):\n",
    "    print('Lasso')\n",
    "    model=Lasso(fit_intercept=True,alpha=0.01)\n",
    "    model.fit(x,y)\n",
    "    y_pred = model.predict(z)**2\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    print('mae',mae,'\\n')\n",
    "    \n",
    "    print('Hist GradientBooster')\n",
    "    model = HistGradientBoostingRegressor(loss='poisson', max_leaf_nodes=128)\n",
    "    model.fit(x,y)\n",
    "    y_pred = model.predict(z)**2\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    print('mae',mae,'\\n')\n",
    "    \n",
    "    print('Poisson Regressor')\n",
    "    model = PoissonRegressor(alpha=1e-12, max_iter=900)\n",
    "    model.fit(x,y)\n",
    "    y_pred = model.predict(z)**2\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    print('mae',mae,'\\n')\n",
    "    \n",
    "    print('DecisionTree Regressor')\n",
    "    model = DecisionTreeRegressor(random_state=2)\n",
    "    model.fit(x,y)\n",
    "    y_pred = model.predict(z)**2\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    print('mae',mae,'\\n')\n",
    "    \n",
    "    print('XGB Regressor')\n",
    "    model = xg.XGBRegressor(objective ='count:poisson',n_estimators = 100, seed = 123)\n",
    "    model.fit(x,y)\n",
    "    y_pred = model.predict(z)**2\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    print('mae',mae,'\\n')\n",
    "    \n",
    "    print('Tweedie Regressor')\n",
    "    model = TweedieRegressor(power=1, alpha=0.5, link='log', max_iter=700)\n",
    "    model.fit(x,y)\n",
    "    y_pred = model.predict(z)**2\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    print('mae',mae,'\\n')\n",
    "    \n",
    "    print('RandomForest Regressor')\n",
    "    model = RandomForestRegressor(random_state=2)\n",
    "    model.fit(x,y)\n",
    "    y_pred = model.predict(z)**2\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    print('mae',mae,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Evaluate the train dataset with estimator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso\n",
      "mae 2.7000140912385038 \n",
      "\n",
      "Hist GradientBooster\n",
      "mae 2.6768701196442013 \n",
      "\n",
      "Poisson Regressor\n",
      "mae 2.6897116279036455 \n",
      "\n",
      "DecisionTree Regressor\n",
      "mae 3.739501707154711 \n",
      "\n",
      "XGB Regressor\n",
      "mae 2.6832680113201897 \n",
      "\n",
      "Tweedie Regressor\n",
      "mae 2.7017446092022634 \n",
      "\n",
      "RandomForest Regressor\n",
      "mae 2.822988564584935 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "estimator(x_train,y_train,x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can see the performance of XGBRegressor and HistGradient Boosting is much better than any other Regressor. We will do hyperparameter tuning with GridSearchCV on XGBRegressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=XGBRegressor(base_score=None, booster=None,\n",
       "                                    colsample_bylevel=None,\n",
       "                                    colsample_bynode=None,\n",
       "                                    colsample_bytree=None, gamma=None,\n",
       "                                    gpu_id=None, importance_type='gain',\n",
       "                                    interaction_constraints=None,\n",
       "                                    learning_rate=None, max_delta_step=None,\n",
       "                                    max_depth=None, min_child_weight=None,\n",
       "                                    missing=nan, monotone_constraints=None,\n",
       "                                    n_estimators=100, n_jobs=...\n",
       "                                    reg_alpha=None, reg_lambda=None,\n",
       "                                    scale_pos_weight=None, subsample=None,\n",
       "                                    tree_method=None, validate_parameters=None,\n",
       "                                    verbosity=None),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'colsample_bytree': [0.5, 0.7],\n",
       "                         'learning_rate': [0.01, 0.1],\n",
       "                         'max_depth': [3, 5, 7, 10],\n",
       "                         'min_child_weight': [1, 3, 5],\n",
       "                         'n_estimators': [100, 200, 500],\n",
       "                         'objective': ['count:poisson'],\n",
       "                         'subsample': [0.5, 0.7]},\n",
       "             scoring='neg_mean_absolute_error', verbose=1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param = {\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'max_depth': [3, 5, 7, 10],\n",
    "        'min_child_weight': [1, 3, 5],\n",
    "        'subsample': [0.5, 0.7],\n",
    "        'colsample_bytree': [0.5, 0.7],\n",
    "        'n_estimators' : [100, 200, 500],\n",
    "        'objective': ['count:poisson']\n",
    "    }\n",
    "\n",
    "gsearch = GridSearchCV(estimator = xg.XGBRegressor(),param_grid=param, scoring='neg_mean_absolute_error',\n",
    "                       cv = 5,n_jobs = -1,verbose = 1)\n",
    "\n",
    "gsearch.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints='',\n",
       "             learning_rate=0.01, max_delta_step=0.699999988, max_depth=7,\n",
       "             min_child_weight=3, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=500, n_jobs=8, num_parallel_tree=1,\n",
       "             objective='count:poisson', random_state=0, reg_alpha=0,\n",
       "             reg_lambda=1, scale_pos_weight=None, subsample=0.5,\n",
       "             tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.641934046345811"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = xg.XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "             colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=-1,\n",
    "             importance_type='gain', interaction_constraints='',\n",
    "             learning_rate=0.01, max_delta_step=0.699999988, max_depth=7,\n",
    "             min_child_weight=3, monotone_constraints='()',\n",
    "             n_estimators=500, n_jobs=8, num_parallel_tree=1,\n",
    "             objective='count:poisson', random_state=0, reg_alpha=0,\n",
    "             reg_lambda=1, scale_pos_weight=None, subsample=0.5,\n",
    "             tree_method='exact', validate_parameters=1, verbosity=None)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)**2 #reversing square root transformation\n",
    "mae = mean_absolute_error(y_test, y_pred.round())\n",
    "mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> With parameter tuned model the mae score is improved(2.641934046345811). We can now fit the model on whole train dataset and predict against test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Hazard'] = train['Hazard']**(1/2) #Square-root transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10200"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train.drop('Hazard',axis=1), train['Hazard'])\n",
    "y_pred = model.predict(test)**2 #reversing Square-root transformation\n",
    "len(y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
